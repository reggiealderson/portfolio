[{
    "title": "Finger on the property pulse - Part One",
    "date": "October 1, 2020",
    "description": "Building a comprehensive property sales database and dashboard",
    "body": "This blog post documents the first stage of a data science project I have undertaken involving the application of New South Wales (NSW) residential property sales data.\nIn this first stage of the project, I have built a program using Python which automates the extraction, transformation, and storage of multiple years’ worth of New South Wales (NSW) property sales transactions.\nThe processed dataset is stored within an SQL database and is linked to a Power BI dashboard which visually portrays the best and worst performing real estate locations in the state. The program can automatically check whether new raw property sales data is available online to be processed – thus keeping the dataset and dashboard up to date.\nComing soon.. Click here to watch me demonstrate how the Python program works, or keep scrolling down to read the full blog post, which details the step by step process I went through to build the dataset.\nComing soon.. Click here to watch me demo the analytics dashboard. I consider this dashboard to be what I have coined, a ‘Minimum Viable Dashboard’ – one which will continue to be improved upon. By no means do I consider it, in its current state, to be fully polished or exhausted of all potential features.\nThe remainder of this blog post provides a brief description of the overall data science project before diving into the processes involved in Part One.\nContents   Project Outline\n Project objectives Part One of the Project Custom dataset requirements Raw data chosen for extraction and transformation    Building The Program\n The program in its final form Disclaimer about the step by step process I went through Step 1: Using Python to extract and transform the raw dataset Step 2: Using SQL to analyse and further transform the new dataset Step 3: Using a geocoding API to enhance the dataset    Building The Dashboard\n Inside Power Query Editor About the Data Model The Minimum Viable Dashboard    Project Outline Project Objectives The aim of this project is for myself to gain theoretical knowledge and practical experience in the following data science disciplines:\n Data modelling (emphasis on reliability, efficiency, and scalability) SQL (Structured Query Language) and relational database management ETL (Extract, Transform, Load) data techniques Interaction with APIs (Application Programming Interfaces) Dashboard design and data analysis using Power BI (emphasis on functionality, applicability, and flexibility) Data visualisation (emphasis on creativity and end user understanding)  Part One of the Project Part One of the project involves building a custom dataset (via an ETL process), stored within a MySQL database and later represented by a data model within Power BI. In the process I will have further developed my Python programming and SQL querying skills.\nThe following four phases are included in Part One:\n Using Python to extract, transform, and store and raw dataset Using SQL to query, analyse and further transform the new dataset Using a geocoding API (Application Programming Interface) to further enhance the dataset Using Power BI to model the dataset and create a ‘Minimum Viable Dashboard’ (MVD)  Later stages of the project:\nContinually build upon the dataset, adding further dimensions and variables, to produce richer insights via Power BI dashboards and reports.\nCustom dataset requirements:  A large dataset spanning thousands of records with a minimum of 10 to 15 labels (columns / independent variables). The dataset should ideally possess time series and geographic components. The dataset should have no monetary cost in acquiring it. The topic and potential insights to be drawn from the dataset should be of personal interest to me. The dataset should reflect a reasonably complex data model.  Raw data chosen for extraction and transformation: After much deliberation and scouring of the internet, hunting for open data sources, repositories, and web scraping inspirations, I stumbled upon data pertaining to records of property sales in NSW.\nThe raw data can be sourced from this website: https://valuation.property.nsw.gov.au/embed/propertySalesInformation\nThe data is uploaded in bulk onto the website using DAT files. There are separate DAT files for each of the 130 geographic districts that a property could be located in. And files are uploaded weekly, meaning that in order to process a full years’ worth of property sales data one would need to collate thousands of DAT files.\nI deemed this raw dataset to be highly appropriate for my project as it met all my requirements. I also have a familiarity and interest in the different suburbs and regions of NSW and working with such a dataset would also allow me to gain knowledge of the real estate market of my own home state.\nI also saw the potential to link the geographic data points (i.e. Suburb, Street, Postcode, District) to other secondary sources of geographic data (such as population, demographics, commercial activity, proximity to other locations, etc).\nPictured below is a screenshot of what one of the DAT files looks like if you open it up in Excel:\n  Building The Program The program in its final form As previously mentioned, I have built a program using Python which automates the extraction, transformation, and storage of property sales data.\nEssentially, with the click of a button, my python script runs the entire data ETL process. If you already had the database set up, and wanted to download and process new raw property sales data so that the dashboard displayed fresh property insights, then the program can do this automatically.\nThe following diagram illustrates how the program works:   Click here to watch me demonstrate how the Python program works, or keep scrolling down to read the full blog post, which details the step by step process I went through to build the dataset.\nDisclaimer about the step by step process I went through The following documentation of the ETL (Extraction, Transformation, Loading) process I undertook does not cover every troubleshooting event and revision of code that I made. To reduce the length of this report, I have not explicitly included any Python code. However, throughout this report I have included links to several notebooks containing all the Python code needed to replicate the project. The online repository for these notebooks can be accessed with this link:\nhttps://github.com/reggiealderson/NSWpropertysales\nBefore finalising the Python scripts featured in these notebooks, I had already gone through several test cycles of extracting, transforming, and loading data, while assessing the cleanliness and completeness of each separate test dataset. There has been much editing of Python code and SQL querying in order to get everything to run efficiently and to minimise the occurrence of missing data.\nThere are imperfections with the raw data (sourced from the NSW Property Website). However, leaving most of these imperfections unattended would not have been a sensible decision if I were to take seriously the validity of future insights drawn from the final custom dataset. In this report I will refer to a few of these imperfections which I have needed to tackle.\nObviously during this initial phase of the project, I did not need to process each property transaction record from multiple years’ worth of property sales data. This report follows the progress of an initial dataset with a sample size reflecting 12 weeks of property sales transactions. This equates to roughly 33,000 individual property sales transactions (in the raw, unprocessed dataset) and was a large enough sample size to allow me to spot any significant flaws in both my code, and the validity and consistency of the raw data. It is also a small enough sample size to prevent unnecessary delays in the processing of data at such a preliminary stage of the project.\nStep 1: Using Python to extract and transform the raw dataset The first step I took in extracting data from the raw DAT files was to identify the data points that I deemed relevant and useful for future analytics reporting. Below is a zoomed in perspective of one of the DAT files viewed in Excel.\n  As you can see it is quite unclear what each cell refers to. In order to make sense of it all I assessed the technical documentation of the raw property sales data to understand the meaning of each column and row. See the below screenshot showing where the raw DAT files and technical documentation are sourced from. As previously mentioned, these files are sourced from this website: https://valuation.property.nsw.gov.au/embed/propertySalesInformation\n  I then created a Python script that inspects each DAT file, and extracts data from only the relevant columns and rows. While creating this script, I simultaneously added code that transformed the raw data into a cleaner format and structure.\nThe linked page below provides my Python script and commentary detailing the extraction and transformation of the raw property sales data. It also acts as a tutorial for anyone wishing to reproduce either an exact replica or similar dataset to the one I created.\nhttps://github.com/reggiealderson/NSWpropertysales/blob/master/Phase1.ipynb\nWriting the Python code featured in this link allowed me to establish an automated and efficient initial ETL procedure.\n☑ In the less than 2 seconds, raw data was extracted and transformed from 1,400 DAT files (equating to 32,968 property transactions and 494,520 data points).\n☑ Data points reflecting each property’s sale value, contract and settlement dates, geographical and address information, size, and type, were stored in a python list format, ready to be stored in a more suitable tangible format such as CSV file or relational database.\n☑ The code was able to remove and ignore all irrelevant data points.\n☑ The code could standardize the data point relating to the size of the property – converting hectares into squared metres wherever there were discrepancies.\n☑ It is now much clearer whether a property is a house or is a unit, whereas the raw data did not have such a simplified Boolean data point to distinguish this.\n☑ Extra code was added to check for missing data points, as well as to delete records with missing data points if need be.\nI subsequently saved the processed dataset into a csv file, ready for upload to a MySQL database.\nStep 2: Using SQL to analyse and further transform the dataset For the 12 weeks of property sales data I was working with, I saved the processed data into a csv file in anticipation to load it into an SQL database.\nI chose MySQL as the relational database management system to handle my dataset and used the accompanying MySQL Workbench software to run SQL queries. It is possible to run SQL queries via Python code, but the Workbench is much easier to use and includes shortcut tools that speed up the process. I would however need to use Python code to load the dataset into a MySQL database.\nFor a more technical documentation of the procedures applied within this phase, please click the link below, which includes SQL and Python code that anyone can copy.\nhttps://github.com/reggiealderson/NSWpropertysales/blob/master/Phase2.ipynb\nIn summary, I created a MySQL database titled ‘testdatabase’ and inside this database I created a table which would exist to store my dataset inside. After consideration and tweaking of the assigned SQL data types that would correspond to each of the 15 data points (columns) per property sales transaction, the following SQL query was written, executed, and subsequently the table, titled ‘property_sales’, which would store the dataset, was created. An extra ‘id’ column was included so that each entry into the table would automatically be given a unique id.\n  Python code was then used to load the dataset into the ‘property_sales’ table of the SQL database.\nInside the MySQL Workbench application, I ran several SQL queries to ensure the correct number of rows were stored in the table, as well as to find and delete duplicate rows. Using SQL queries, I found and removed 4,314 duplicate entries. This figure equates to a 13% reduction to the size of the dataset. I was incredibly surprised the raw dataset included such a high proportion of duplicate entries.\nIn the subsequent, streamlined ETL process (in which the whole ETL process can be automated with the click of a single button), I use Python code to remove duplicate entries (as this method takes a matter of seconds to process as opposed to several minutes using MySQL).\nStep 3: Using a geocoding API to enhance the dataset While querying the data in MySQL Workbench I noticed that the number of unique district codes featured in the dataset was greater than the number of districts listed in the technical documentation for the raw data (linked below).\nhttps://www.valuergeneral.nsw.gov.au/__data/assets/pdf_file/0018/216405/Property_Sales_Data_File_District_Codes_and_Names.pdf\nI also noticed the following warning in the documentation:\n“A property’s recent district code and district name may change over time due to council boundary alignment and council mergers. Accordingly, district references for sales data files may change from year to year.”\nEssentially, this means that the district codes given for each property transaction are unreliable given they change over time. And I could not find any version control resources (relating to changes in district codes over time) available from NSW government websites.\nI wanted to use a property’s geographical district as a filter/segment in my future analytics reporting and exploratory data analysis. So, I was not going to ignore this problem.\nUnfortunately, I could not simply reassign all rows in the dataset to a revised district code based on the value in the suburb or postcode fields. This is because many suburbs and postcodes span multiple districts. As evidenced by the results produced from the following SQL queries:\n    It is also worth mentioning that on another test run I did for this project, where I used 8 weeks’ worth of property sales transactions data where the settlement dates for each property occurred in 2018, that the discrepancies in district codes and suburb / postcode data was even more pronounced.\nThis non-uniform characteristic relating districts with suburbs and postcodes is further verified by a quick Wikipedia search for some of these suburbs. See an example screenshot below where I have highlighted in a pink box the indication of multiple districts per one suburb.\n  However, I came up with a solution. I found the NSW Government’s Address Location Web Service (linked below), which is an API (Application Programming Interface) that anyone can interact with via a web browser. Essentially it allows you to enter certain locational parameters pertaining to a property in NSW, and the API returns a JSON object (which is basically a simplified data container) that holds a bunch of information pertaining to the property – including its designated Local Government Area (district).\nhttp://maps.six.nsw.gov.au/sws/AddressLocation.html\nBy creating a Python script that can interact with the API, I could automate the retrieval of the correct up-to-date district name for a property in the dataset.\nThe API required clean inputs for both a property’s street name and suburb. Included in my Python script is several lines dedicated to transforming the data points relating to house number, street name, and suburb name – in order to transmit accurate and acceptable data to the API.\nBecause there are numerous data entry mistakes relating to these data points in the raw data, I could not achieve 100% success rate in terms of generating responses from the API.\nHowever, I managed to retrieve an accurate designation of district names for 99.56% of the dataset. I figured the loss of 0.44% would not affect reporting and statistical inference for most of the relevant analytic measures I would use in my Power Bi dashboards and reporting.\nFurthermore, I also used the API to retrieve latitude and longitude coordinates for each property, as these data points might come in handy later when I experiment with my analysis in Power BI.\nThe linked page below provides my Python script and commentary detailing the process I went through in this phase of the project:\nhttps://github.com/reggiealderson/NSWpropertysales/blob/master/Phase3.ipynb\nTo summarise, this phase of the project achieved the following:\n☑ Automated the retrieval of geocoding data for 28,526 properties, representing 99.65% of the total number of properties in the dataset. This API retrieval processing took 45 minutes using Python code.\n☑ Created a cleaned street name variable for each property transaction in the dataset. This new variable (which is a string containing the street name without any unnecessary characters or road type words/abbreviations) could be passed through the Address Location Web Service API with a higher success rate in terms of generating district and longitude/latitude coordinates.\n☑ This new street name variable can also be used to better filter Power BI charts and tables – if one wanted to filter by street name in their reporting.\nIn my case I loaded the new geographical data into a new table in my SQL database. See the screenshot below where I run a few SQL queries to verify and analyse the new geocoding data.\nChecking that the new table (titled ‘geocoding_data’), which stores the new geocoding data, contains the same number of rows as the pre-existing ‘property_sales’ table:\n  28,651 was indeed the figure I was looking for.\nWhat if we wanted to see the number of distinct district names in our database versus the number of distinct suburb names and distinct postcodes?\n  Interesting… So, there are 128 distinctive districts in the dataset, 569 distinct postcodes, 2,058 distinct suburbs, and 9,377 distinct street names. Of course, identical street names could feature across multiple suburbs.\nI wonder what street name is the most common in terms of how many districts it is found in.\n  Looks like Queen Victoria spread her name as vastly as she could.\nAnd what is the hottest street on the market when it comes to volume of property sales? My guess is Kerr Avenue in Miranda.\n  Looks like the Mayor of Ryde likes his/her zoning regulations nice and loose.\nAnd which district has the highest average property sale value? Let’s run two separate queries. One for transactions pertaining to the sale of units, and one pertaining to the sale of house. We’ll also make sure our sample sizes are at least n=50 so as to improve validity.\n  The Northern Beaches tends to rank higher for average unit sale price relative to house sale price. Maybe one pays more for the views one might get from living in a high-rise apartment building near one of the many beaches on the Northern beaches. We could go on further and look at the size of the apartments, the exact suburb (whether it is close to the coastline or further inland), etc. But my main mission was to confirm the new geocoding data is stored correctly and swiftly move onto Power BI!\nBuilding The Dashboard The remainder of this blog post discusses the use of Power BI to model the dataset and create a Minimum Viable Dashboard.\nInside Power Query Editor I used the Power BI Power Query Editor to transform the dataset in preparation for efficient querying and analytics reporting via the dashboard.\nThe following steps were taken inside the editor:\n Launch Power Query Editor and load in the two tables from the MySQL database Select the Transactions table and click \u0026lsquo;Merge Queries as New\u0026rsquo; in the Combine section of the Home tab. Within the new merged query, remove unnecessary columns, rename columns so they are understandable and are without underscores, and set up all necessary dimension columns are set up. One by one, duplicate the merged query for each dimension table and fact table and remove all irrelevant columns. For dimension tables, select the column with the fewest unique values, then select remove duplicates, and then create an index column within the dimension table that will later be linked to the main fact table/s. Create two separate date dimension tables (one for the Settlement Date dimension, the other for the Contract Date dimension).  About the Data Model Here is the data model I ended up with within Power BI:\n  This model resembles a star schema, which is a model that classifies each table into one of two types – dimension tables and fact tables. Dimension tables contain columns (variables) that are used to filter and group data, while fact tables contain columns that represent transactions or observations, such as purchases, sales, requests, etc.\nIn the case of my data model, there is one fact table, and several dimension tables. The fact table is named ‘Transactions FACT’ and stores the numeric event data including purchase prices, dates, property size, and longitude and latitude coordinates. Also contained within this fact table are multiple ID columns known as surrogate keys and are used to provide a unique identifier for each dimension table row.\nThe dimension tables each surround the fact table, providing ways to filter and slice the numeric data in the fact table. For example, the Location DIM table stores categorical data pertaining to a property’s geographic traits. A one-to-many relationship links the Location DIM table to the Transaction FACT table in a way that for every property transaction there can only be “one” distinct selection in each of the Location DIM table’s column variables, while for each of the DIM table’s column variables there could be “many” potential properties. I.e. the Post Code ‘2250’ could be linked to many properties, but only one Post Code can be assigned to each property sale in the Transactions FACT table. Note that there also exists a table named ‘SORT Regions’ linked to the District DIM table. This is a special table that helps to sort the Region categories in a tailored manner.\nIf this were a data model used for transactional database, the star schema model type would potentially be unsuitable, because if changes needed to be made to certain categorical variables, such as the naming of a certain Suburb, or if a Post Code were to be added, then it would be better to have organised the tables in a way that reflects proper data normalization – which typically increases the number of dimension tables in the model, and splits columns within a denormalized dimension table into multiple normalized tables so that the transactional database is built to reduce redundancy and increase data consistency. But because Power BI is used for reporting, as opposed to performing CRUD (Create, Retrieve, Update, Delete) operations, we can denormalize the dimensions, and increase query efficiency.\nIt is worth noting that within my data model, the column storing the District categories is held within a separate dimension table to that which holds suburb, post code, and street categories. This was a subjective decision made by me to split these dimensions apart. I wanted to allow for easier interpretation of the model, as I will potentially add more columns related to Districts – such as District population, resident income levels, and other demographic data. I would rather keep this information separate from the Location DIM table, and because the District column possesses low cardinality (few distinct values), I feel as though not to worry about the potential query delays from added an extra normalized dimension table to the model.\nThe Minimum Viable Dashboard I have produced what I have coined as a ‘Minimum Viable Dashboard’ – one which will continue to be developed upon and be further enhanced. By no means do I consider it, in its current state, to be fully polished or full of all potential features. I wanted it to be good enough so that someone working within the real estate industry / any residential property related field could use it and gain practical insights that they otherwise would have had to pay for.\nPictures below is a screenshot of the dashboard’s user facing interface:\n  From a User Interface standpoint there are two broad elements built into the dashboard. Those being filters and the other being visualisations. There are three main filters positioned at the top of the interface, one for filtering the data in the visualisations by date, one for filtering by property type, and one for filtering by sample size. A fourth, and less important filter, is displayed in the bottom left of the interface, which is used to filter the data by three large geographic regions – useful, for instance, in comparing districts with each other that are all located within the Greater Western Sydney region.\nThere are three data visualisations. The one on the left is a Shape Map, which visually contrasts the differing districts in New South Wales. With this visual the user can make several interactions including zooming in and out, selecting a district to focus the other two visualisations, and hovering over each district to provide various statistics pertaining to that district. The second visualisation is a table, located in the top right-hand side of the interface. This gives precise figures for each of the key metrics pertaining to the overall theme. I should mention that the overall theme is a comparison in property prices between two periods – as denoted by the title located in the top left-hand side of the panel. The third visualisation is a scatterplot and is used to visually compare four different metrics all within the same two-dimensional space. The Y and X axis of the scatterplot convey Median Sales Price (Current Period) and the % Change in Median Sale Price. The two differing shades of blue for each dot separate each district as either one which has had an increase in sale price or a decrease. The size reflects the absolute value in the change in median sale price.\nI have ensured all elements, including colour schemes, are labelled, so as to give an immediate explanation to the user. The sample size filter is an important element in my opinion – and one which I feel is underutilised in dashboard design (from what I gather during my research). The importance comes from the potential to be misled by distinctions in colours and sizes of bars when comparing each district. A district with less than 10 transactions for a selected date period can produce wild swings in median sale price – thus standing out in the visualisations, which is not ideal if validity of insights is of importance to the end user.\nClick here to watch me demo the analytics dashboard, or scroll to the end of this blog post to read about the data model design and Power Query Editor steps I took within Power BI prior to creating the dashboard.\nThis ends Part One of my series of Finger on the Property Pulse posts. I aim to produce more insights and dashboard designs to share! Please let me know if you like what you have seen or have any requests.\n",
    "ref": "/blog/201001proj/"
  },{
    "title": "Why we watch the highlights",
    "date": "July 17, 2020",
    "description": "Examining the factors determining viewership of rugby league match highlights",
    "body": "What makes us watch online video highlights of sporting events?\nIs it our desire to see an underdog snatch victory from the hands of a giant?\nIs it our pleasure in viewing all scoring events in one small package?\nIs it our love for a certain team?\nIs it a nail-biting score line?\nOr is it something else?\nIn this report I present an analysis on the topic of the National Rugby League (NRL) and the factors which determine the viewership ratings of NRL match highlights videos on YouTube.\nClick here to watch my video presentation of the report or scroll down to read the full report\nContents   Project Background\n  Project Methodology: Data Collection and Preparation\n Step 1: Extracting YouTube Data Step 2: Removing cases from the YouTube dataset Step 3: Sourcing independent variable data    About the Dataset\n  Analysis\n  How much variation in viewership is there for NRL match highlights videos on YouTube?\n  Examining the outliers\n  How do view counts differ based on the popularity and strength of the teams?\n  How do view counts differ based on the expected result of a match?\n  Do view counts differ based on score line and number of points scoring events?\n  Do view counts differ based on time of the year and/or based on Channel Nine broadcasting?\n    Modelling the Relationship between Video Views and determining factors\n  Key Insights\n  Recommendations to Stakeholders\n  Project Background This report documents the findings of a June 2020 study examining data pertaining to National Rugby League (NRL) matches from the 2019 NRL season.\nThe purpose of this study is twofold:\n Gain an understanding as to what influences viewership of NRL match highlights Provide actionable insights for operators within the NRL to effectively influence strategic decision making  Project Methodology: Data Collection and Preparation Extracting YouTube Data To understand the factors influencing the viewership numbers of NRL match highlights, data from YouTube was extracted and used as both dependent and independent variables in the analysis of this project.\nOver the course of the 2019 NRL season the NRL on Nine YouTube channel uploaded highlights videos of each NRL match from the regular season fixture list (excludes finals matches). Each match highlights video was uploaded on the channel within a few hours of the completion of the corresponding live match.\nPhase 1 of this project involved extracting the viewership numbers of these videos, as of June 2020.\nThe specific method to extract this data involved the use of the YouTube Data API and various programming functions within Python.\nAs a result of this data extraction process, data from 192 YouTube videos (pertaining to 192 regular season NRL matches) was consolidated into both Python and Excel.\nRemoving cases from the YouTube dataset One unfortunate characteristic of the dataset extracted from YouTube pertains to the viewership restriction settings for each video. 187 of the 192 videos are restricted to Australian viewers only. Thus, the 5 videos which do not have this restriction, were removed from the analysis.\nFurthermore, because New Zealand viewers are unable to view the videos, it is assumed that viewership numbers of videos featuring the New Zealand Warriors team will be significantly affected.\nThus, all videos of New Zealand Warriors matches were removed from the analysis.\nA total of 163 games remained as viable datapoints for analysis.\nSourcing independent variable data Besides YouTube data, data pertaining to match and team statistics were sourced via web scraping various websites and consolidating the data into data frames within Python.\nThe predominate source of match data came from Fox Sports.\nFurthermore, custom data metrics were created within Python and Excel by transforming the primary data scraped from websites such as Fox Sports.\nAbout the Dataset 163 games (cases) are included in the analysis.\nThe ‘YouTube video views’ metric is portrayed throughout this report and represents the amount of views a match highlights video received on the NRL on Nine YouTube channel (as at June 2020).\nIn this report I explore the relationship between the YouTube video views variable and the following independent variables:\n Supporter levels of each team The strength (recent results) of a team going into a match The degree to which the winner of a match was expected to win Number of tries scored in a match Whether a field goal was scored in a match The final score-line margin of a match The biggest score-line margin throughout a match The number of tries/field goals scored in the last 15 minutes of a match The specific round a match is held in The match attendance figure Whether or not the match was televised live on Channel Nine The durational length of the match highlights video  Analysis How much variation in viewership is there for NRL match highlights videos on YouTube?   The YouTube video views dataset does not resemble a normal distribution, as clearly evidenced by the normal probability plot (green line) displaying an upwards concavity relative to what we would expect from a normal distribution (purple line).\n  Observing the above histogram, the majority (85%) of videos have less than 100k views. And quite a large proportion (63%) of videos have between 50k and 80k views.\nThe dataset is heavily skewed to the right due to several extreme outliers existing, including one video with 366k views.\nThere is nothing bad per se about a non-normal distribution. It just means that if these observed values were to be used to predict future video view values using a regression model then the outliers would need to be monitored carefully in relation to the model output generated.\nExamining the outliers   I have deemed there to be six major outlier data points in the dataset and have brief characteristics of each outlier are listed above.\nScanning the characteristics of each outlier, you can see there are a mixture of score lines and ladder positions, although the biggest outlier features a 1-point final score line margin and has the top two rank teams versing each other. The outliers (the six matches) all occurred between rounds 6 and rounds 11.\nUpon watching these videos, and through the lens of a hardened rugby league consumer, you will notice that a lot of quality and unique tries were scored. Furthermore, in-game crowd attendance and engagement are relatively high.\nAn understanding of the characteristics of the outliers in the dataset helped to steer the direction in which I took my exploratory analysis as well as when drawing conclusions and recommendations.\nHow do view counts differ based on the popularity and strength of the teams?   Featured above is a scatterplot with each dot representing one of the 163 matches in the dataset. The horizontal axis measures the YouTube video viewership counts for each match and the vertical axis is what I have coined the Supporter Index, which measures the level of support for the two teams featured in each match.\nThe supporter index is calculated by using each club’s official Facebook page likes count (as at June 2020) and 2018 end of year club membership numbers. The index score is a standardized value in which the Facebook page likes value is given a 90% weighting and the membership numbers a 10% weighting.\nThere exists a moderate positive correlation (+0.35) between video viewership numbers and the supporter index. This correlation score increases to +0.46 upon removing the six outliers (coloured purple) from the calculation.\nIn other words, looking across different matches, as the supporter levels of the teams involved in a match increases, views tend to also increase – but the relationship is only moderate. Which makes sense, as you would expect that more heavily supported teams drive higher video views but that it could not be the sole factor that drives views.\n  The Broncos are clearly the most supported club in the competition, while the Storm and Rabbitohs are the closest challengers. Which begs the question, do match highlights featuring the Broncos have the highest view counts?\n  The answer is, well sort of but not really. Games which feature either the Roosters, Broncos, or Storm typically get around 86k views. One big question is how do the Roosters and the Storm get as many views as the Broncos despite having smaller supporter bases\u0026hellip;\nWhat is also interesting is the differences in views based on whether the team was the winner or the loser in a match. Both Roosters and the Storm appear to draw a bigger number of viewers when they are the losing teams in a match as opposed to when they are the winner. To me this suggests that viewers are especially interested in seeing a traditional powerhouse team lose.\nSpeaking of powerhouses and underdogs, let us now look at the relationship between video views and the strength of the teams involved in any given match.\n  Illustrated above is a new scatterplot, with the horizontal axis again measuring the viewership counts for each match. However, the vertical axis now measures each match based on what I have coined the Match Strength Index. The strength index is based on a team’s most recent results leading up to the match in question. The better a team has performed in its recent matches leading into a new match, the higher its strength index score will be.\nAgain, like with the supporter metric, we see a moderate positive correlation (+0.40) between match strength index and YouTube video views. A slightly stronger correlation (+0.44) exists when the outliers are removed from the calculation.\nNow that we have seen the bivariate relationship between supporter levels and views, and strength levels and views, which teams are underdelivering and which teams are overdelivering? What I mean by underdelivering and overdelivering is circumstances where a team has either lower or higher levels of views on their match video highlights relative to their supporter and strength scores.\n  Assessing the above graph, the teams where this phenomenon occurs to a noticeable degree are with the Storm and the Rabbitohs, where both these teams\u0026rsquo; views scores are lower than the strength and supporter scores. This suggests these two teams are not delivering as many YouTube video views for their games as they should be. On the flip side, you could also argue that the Knights, and to a lesser extent the Eels, are the only teams overdelivering. This could indicate the supporters of these two teams are more engaged with their teams\u0026rsquo; match highlights. It could also indicate that these teams possess individual players that, on average, supporters of opposing clubs deem more worth watching.\nHow do view counts differ based on the expected result of a match?   Illustrated above is a new scatterplot, showing again video view counts on the horizontal axis. On the vertical axis is a new metric, which I have coined the Match Upset Index. A positive match upset index score reflects any given match in which the winning team were expected (based on team strength index) to lose prior to the match. And a negative match upset index score reflects any given match in which the winning team were expected to win.\nThe resulting correlation between video views and match upset scores is a low positive correlation (+0.13). This suggests that as a match becomes more of an upset, it is slightly more likely than normal for video views to increase.\nWhat we have discovered is that the match upset factor is not as influential as the supporter and strength index factors when it comes to determining video views.\nDo view counts differ based on score line and number of points scoring events?   One might expect videos with more tries and field goals to generate more views. However, the relationships here are not as strong as we saw with the supporter and strength metrics. An increase in tries scored in a match typically yields a relatively small increase in views (correlation of +0.07), while a slightly stronger positive correlation (+0.22) exists based on whether a field goal was scored or not.\nDoes the margin of victory influence viewership levels?\n  The correlation between video views and the score-line margin is weak (+0.07), however there may be a nonlinear relationship pertaining to relatively low margins and relatively big margins. Matches with final margins less than 4 points and matches with final margins greater than 23 points appear to draw in more views than those matches with final margins between 4 and 23 points. In other words, consumers are more likely to watch a video if the match in question had a tight score line or a massive score line.\nOn the right-hand side chart featured above we have video views based on the largest points lead held in a match by either the winning or losing team in each match. There does not appear to be any kind of pattern going on here that helps us understand viewership rates better. Which is somewhat surprising given that I would have expected matches that held a close score-line margin throughout the match to be more engaging and thus generate more views.\nDoes the occurrence of multiple late scoring tries and field goals increase views?\n  An increase in the amount of tries and field goals scored after the 65th minute of a match does not necessarily result in increased video views.\nThe correlation between YouTube video views and the number of tries/field goals scored after the 65th minute of a match is a measly +0.04. However, matches featuring no tries or field goals generated less views than matches that do feature late scoring events.\nIt would appear beneficial for a highlights video package to feature at least one late scoring event but having more than one does not guarantee higher views.\nDo view counts differ based on time of the year or on Channel Nine broadcasting?   On a per round basis, YouTube video views peaked in Round 9, a round in which all games were played at Brisbane’s Suncorp Stadium.\nWhat was it that made the magic round so viewable? I would argue it was because the actual games were attended by more people.\n  Crowd figures were higher than average in round 9, and there exists a moderate positive correlation (+0.41) between crowd figures and YouTube video views.\nIs it that the video highlights came across as more exciting to watch on YouTube due to the larger crowds featured in the highlights? Or is it that teams demonstrated a more attractive style of play that round due to the increase in fans watching? I would lean more towards the first explanation.\nWhat about Channel Nine coverage? After all, the YouTube videos being analysed are uploaded by a channel which is owned and operated by Channel Nine. The video uploaders may be more inclined to promote video highlights of matches that were originally broadcasted on Channel Nine.\n  The above graph features box and whisker plots for two segments of data. The segments are based on whether a match was televised on Channel Nine. The box on the left (representing videos of matches which were not originally televised on Channel Nine) is clearly lower than the box on the right, indicating that the distribution of this segment is skewed more towards lower video view values. The boxes themselves reflect the middle 50% of the data points in each of the segments while the upper and lower “whiskers” reflect the minimum and maximum values (excluding outliers). The dots above the box and whiskers reflect the outliers in each segment.\nThe following table provides further distinguishing characteristics of the two segments:\n  Televised games had a significantly higher amount of views on their YouTube highlights. In my opinion this is because Channel Nine generally picked matches to televise that featured on average more heavily supported teams, as the data in the table shows. Matches that were televised on Channel Nine also had longer durations for their YouTube video highlights (perhaps due to a contractual agreement set in place between broadcasters and the NRL). My hypothesis would be that longer video durations lead to higher views on average, based on my theory that consumers appreciate seeing a wider variety of moments from any given match.\n  Video duration does seem to have a connection with video views as there exists a moderate positive correlation (+0.37) between the two variables. And for those wondering, the correlation between duration and match strength is much weaker (+0.11).\nModelling the Relationship between Video Views and determining factors Throughout this report, univariate and bivariate analysis has been presented for the purposes of exploring the factors influencing viewership numbers of YouTube NRL match highlights videos.\nUsing the insights gained thus far, a multivariate statistical model was created, portraying the relationship between YouTube video views and independent factors.\nThe model is based on the general statistical technique known as multiple regression analysis.\nThe main objective of multiple regression analysis is to predict future values of a single dependent variable using known values of multiple independent variables.\nHowever, for this project I have simply used this technique to answer the following two questions:\n How much of the variation in viewership can be explained with a statistical model? Which factors are best at explaining the variation in viewership numbers?  Using the variables/metrics detailed throughout this presentation, I experimented with various combinations of independent variable inputs to construct a multiple regression model that maximises the power to predict YouTube video view figures.\nI ensured no violations of multiple regression validity assumptions were made (including the prevention of multicollinearity). For the purposes of adhering to these validity assumptions, the dependent variable (YouTube video views) needed to be transformed via a logarithmic transformation, and the six major outlier datapoints in the dataset (discussed in the beginning of this report) were removed.\nEach independent variable was normalized (each value scaled to a value equalling between 0 and 1), so that the absolute values of the regression coefficients could be compared for the purposes of determining which independent variables are more important in explaining the variation in video views.\nAfter testing several different multiple regression models, the best combination of factors (out of those featured in this report) at determining video viewership variation, are the following four factors:\n  Match strength index - The degree to which a match features teams who have performed well over recent fixtures.\n  Match supporter index - The combined relative supporter bases of the two teams featured in a match.\n  Video duration - The durational length (measured in seconds) of the YouTube video.\n  Crowd attendance - The crowd attendance figure for a match\n  A regression model, with these four factors acting as independent variables, accounted for 44% of the variation in YouTube video viewership numbers. This % figure derives from the Adjusted R Square statistic from the regression model output in Excel.\nThe match strength index is the most powerful predictor, accounting for 41% of the model’s predicting capability. The match supporters, video duration, and crowd attendance factors account for 22%, 22%, and 16% of the model’s predictive capability, respectively.\n  Key Insights   Differences in the form of a team, supporter levels of a team, video duration, and match crowd attendance contribute most to the variation in viewership levels.\n  Combined with the above factors, and upon reviewing the major outliers in the dataset, the following wildcard factors appear to drive extreme increases in view counts:\n  Individual brilliance. E.g. Kalyn Ponga’s flamboyant line break and try in the Round 9 Bulldogs v Knights fixture.\n  Periods of exhilarating attacking play. E.g. Several fast-paced long-range tries featured in the Round 9 Storm v Eels fixture.\n    Views are particularly high for matches in which heavyweight teams lose. Matches which featured either the Roosters or the Storm losing garnered much higher views than other games on average.\n  Given that the duration of a video influences view counts, whereas the amount of try scoring events does not, it would seem consumers are interested in seeing a variety of different highlights as opposed to the same type of highlight over and over again.\n  Recommendations to Stakeholders   Inject more uniqueness and novelty into the promotion of seasonal fixtures.\n The ‘Magic Round’ featured in Round 9 attracted a higher than average amount of video views. Thus, it would make sense to add more novel elements to other rounds. E.g. A round where all matches are played overseas. E.g. A round where all teams wear strikingly different jerseys (like the matches where teams wore jerseys with superhero designs).    Add more narrative style audio-visual elements to the highlights packages. For instance:\n  Include video intros that focus on enhancing the magnitude of a match prior to showing the actual match highlights. These could be short 15 second audio bites overlaid on top of footage of the players running out onto the ground.\n  Have the voiceover commentators refer to club rivalries.\n  Show passionate supporters in the crowd.\n  Highlight individual player matchups while exaggerating the unique talents of certain players.\n  Show subtle visual prompts updating the score-line and timepoints of each separate highlight.\n    Emphasis should be placed on the uniqueness and variety of highlights shown in each YouTube video, as opposed to showing a video which just seems to just repeatedly show each try scoring event in a back-to-back sequence.\n  Further analysis of YouTube video views should incorporate the following metrics:\n  A measurement relating to the characteristics of the commentary on each video. E.g. Which commentators are featured in a match, and how memorable is their input.\n  Types of tries scored. E.g. How many long-range tries are scored, how many tries from kicks are scored, etc.\n  Player popularity and number of times a popular player scores or has a considerable impact in a game. E.g. How many times in a match did Kalyn Ponga score or assist a try?\n  Number and type of non-scoring events featured in video. E.g. Did a highlights video feature a heated dispute between several players?\n  View counts should ideally be measured at a specific and consistent timepoint. I.e. 72 hours after the completion of each match.\n    ",
    "ref": "/blog/200717proj/"
  },{
    "title": "How to build a rich dataset using web scraping with Python",
    "date": "July 14, 2020",
    "description": "How to easily create a comprehensive sports statistics database by writing a programming script that can scrape data from multiple web pages with the click of a button",
    "body": "In this video tutorial I demonstrate how to create a sports statistics database using the simple, yet powerful data extraction technique known as web scraping. This technique involves collecting data across multiple web pages without having to open a web browser.\nTutorial:   Code used: \r# How to efficiently create a comprehensive sports statistics data set using web scraping and Python\r# Install the following python modules\r#1 pip install urllib3\r#2 pip install bs4\r#3 pip install regex\r#4 pip install pandas\rfrom urllib.request import Request #1 opening website to scrape\rfrom urllib.request import urlopen #1 opening website to scrape\rfrom bs4 import BeautifulSoup as soup #2 webscraping HTML data\rimport re #3 to parse html text\rimport pandas as pd #4 storing data in data frames\r# https://www.foxsports.com.au/nrl/nrl-premiership/match-centre/NRL20200101\rallmatchcodes_list =[]\rmatchcodes = ['0101','0102','0103','0104','0105','0106','0107','0108','0201','0202','0203','0204','0205','0206','0207','0208']\rfor matchcode in matchcodes:\rurl_to_scrape = 'https://www.foxsports.com.au/nrl/nrl-premiership/match-centre/NRL2020' + matchcode\rreq = Request(url_to_scrape, headers={'User-Agent': 'Mozilla/5.0'})\rwebpage = urlopen(req)\rpage_soup = soup(webpage.read(), \u0026quot;html.parser\u0026quot;)\rhome_team_data = page_soup.findAll(\u0026quot;div\u0026quot;, {\u0026quot;class\u0026quot;: \u0026quot;styles__TeamName-sc-1fpm22n-6 cYFnvo\u0026quot; })\rhome_team_data_str = str(home_team_data)\rHomeTeam_str = (re.findall(r'(?\u0026lt;=\u0026gt;)[^\u0026lt;:]+(?=:?\u0026lt;)',home_team_data_str)[0])\raway_team_data = page_soup.findAll(\u0026quot;div\u0026quot;, {\u0026quot;class\u0026quot;: \u0026quot;styles__TeamName-sc-1fpm22n-6 hgzvAy\u0026quot; })\raway_team_data_str = str(away_team_data)\rAwayTeam_str = (re.findall(r'(?\u0026lt;=\u0026gt;)[^\u0026lt;:]+(?=:?\u0026lt;)',away_team_data_str)[0])\rscore_data = page_soup.findAll(\u0026quot;div\u0026quot;, {\u0026quot;class\u0026quot;: \u0026quot;styles__MatchScore-sc-3rdpqd-0 fcKRjY\u0026quot;})\rscore_data_str = str(score_data)\rscores_str = (re.findall(r'(?\u0026lt;=\u0026gt;)[^\u0026lt;:]+(?=:?\u0026lt;)',score_data_str)[0:3])\rHome_team_score = float(scores_str[0])\rAway_team_score = float(scores_str[2])\rmaindata = [matchcode, HomeTeam_str, AwayTeam_str, Home_team_score, Away_team_score]\rmaindata_df = pd.DataFrame(maindata).T\rmaindata_df = maindata_df.rename(columns={0: \u0026quot;Matchcode\u0026quot;, 1: \u0026quot;HomeTeamName\u0026quot;, 2: \u0026quot;AwayTeamName\u0026quot;, 3: \u0026quot;HomeTeamScore\u0026quot;, 4: \u0026quot;AwayTeamScore\u0026quot;})\rtypes_of_scoring = page_soup.findAll(\u0026quot;th\u0026quot;, {\u0026quot;class\u0026quot;: \u0026quot;styles__EventHeader-sc-18qpy7g-2 iJwCdf\u0026quot;})\rtypesofpts_featured_list = []\rfor i in types_of_scoring:\rstring_a = str(i)\rstring_b = string_a.replace(\u0026quot; \u0026quot;, \u0026quot;\u0026quot;)\rextracter_a = (re.findall(r'(?\u0026lt;=\u0026gt;)[^\u0026lt;:]+(?=:?\u0026lt;)',string_b)[0:10])\rtypesofpts_featured_list.append(extracter_a)\rtypesofpts_featured_list_new = []\rfor i in typesofpts_featured_list:\rstring_aa = str(i).strip(\u0026quot;[]\u0026quot;)\rstring_bb = str(string_aa).replace(\u0026quot;'\u0026quot;,\u0026quot;\u0026quot;)\rtypesofpts_featured_list_new.append(string_bb+\u0026quot;_HT\u0026quot;)\rtypesofpts_featured_list_new.append(string_bb+\u0026quot;_AT\u0026quot;)\rscoreline_changes = page_soup.findAll(\u0026quot;td\u0026quot;, {\u0026quot;class\u0026quot;: \u0026quot;styles__EventData-sc-18qpy7g-1 eyWugu\u0026quot;})\rmatchscorelinelist = []\rfor i in scoreline_changes:\rstring_aaa = str(i)\rstring_bbb = string_aaa.replace(\u0026quot; \u0026quot;,\u0026quot;\u0026quot;)\rextracter_aaa = (re.findall(r'(?\u0026lt;=\u0026gt;)[^\u0026lt;:]+(?=:?\u0026lt;)',string_bbb)[0:500])\rextract_bbb = extracter_aaa[1::3]\rmatchscorelinelist.append(extract_bbb)\rdf_scoreevents_a = pd.DataFrame(matchscorelinelist)\rdf_scoreevents = df_scoreevents_a.T\rdf_scoreevents.columns = typesofpts_featured_list_new\rdf_scoreevents_counts_a = pd.DataFrame(df_scoreevents.count(axis='rows'))\rdf_scoreevents_counts = df_scoreevents_counts_a.T\rdataframes = [maindata_df, df_scoreevents_counts]\rdf_combined = pd.concat(dataframes, axis=1)\rallmatchcodes_list.append(df_combined)\rall_matches_df = pd.concat(allmatchcodes_list).fillna(0)\rall_matches_df['Final_Scoreline_Margin'] = abs(all_matches_df['HomeTeamScore'] - all_matches_df['AwayTeamScore'])\rall_matches_df.to_csv(r'C:\\Desktop\\all_matches_df.csv', index = False, header=True)\r",
    "ref": "/blog/200714proj/"
  },{
    "title": "About me",
    "date": "July 10, 2020",
    "description": "Hey! My name's Reggie. I am a researcher and data analyst. This website showcases my personal data science portfolio and other creative projects.",
    "body": "My name is Reggie Alderson and I am an experienced analyst based in Sydney, Australia. I have held corporate positions in both Market Research and Public Relations.\nIn my previous role as Lead Analyst at RFI Group, I managed multiple syndicated and bespoke quantitative and qualitative research projects for several of Australia\u0026rsquo;s leading financial institutions.\nI have dedicated the majority of 2020 to gaining data engineering skills by working on my own personal data science projects.\nI am fascinated by ideas, patterns, systems, and philosophies. I am a strong advocate of education through experience and exposure to different real world environments (as opposed to classroom and textbook based learning). In 2019/2020 I spent six months travelling and living overseas.\nTeaching English in Poland (feat. my two star students Tadeusz \u0026amp; Conrad):   Bumping into Batman while snowBoarding in Bansko, Bulgaria:   Bike riding in Prague:   Los Österreich! Acting as Marko Arnautovic\u0026rsquo;s biggest fan before a football match in Vienna:   On the hunt for the best matcha treats in Japan:     ",
    "ref": "/about/"
  },{
    "title": "Contact me",
    "date": "July 10, 2020",
    "description": "",
    "body": "Contact me via LinkedIn or via email.\n",
    "ref": "/contact/"
  }]
